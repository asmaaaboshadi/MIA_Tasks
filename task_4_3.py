# -*- coding: utf-8 -*-
"""Task_4.3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UKa_hl3j_SOlOBXEYju1l46JnFaKxzC1
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Provide the path to the dataset
file_path = '/content/drive/My Drive/Asmaa_Task_4/weatherHistory.csv'

# Load the dataset
df = pd.read_csv(file_path)
df.head()

# Separate numeric and non-numeric columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
non_numeric_cols = df.select_dtypes(exclude=['float64', 'int64']).columns

# Fill missing values in numeric columns with the mean
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# Handle missing values in non-numeric columns (e.g., fill with mode or drop)
# Here, we'll fill missing values in non-numeric columns with the mode
for col in non_numeric_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Example: Handle incorrect data (e.g., outliers, incorrect types) for all columns
df = df[(df['Temperature (C)'] >= -50) & (df['Temperature (C)'] <= 50)]  # Temperature range
df = df[(df['Apparent Temperature (C)'] >= -50) & (df['Apparent Temperature (C)'] <= 50)]  # Apparent Temperature range
df = df[(df['Humidity'] >= 0) & (df['Humidity'] <= 100)]  # Humidity range
df = df[(df['Wind Speed (km/h)'] >= 0) & (df['Wind Speed (km/h)'] <= 200)]  # Wind Speed range
df = df[(df['Wind Bearing (degrees)'] >= 0) & (df['Wind Bearing (degrees)'] <= 360)]  # Wind Bearing range
df = df[(df['Visibility (km)'] >= 0) & (df['Visibility (km)'] <= 50)]  # Visibility range
df = df[(df['Pressure (millibars)'] >= 800) & (df['Pressure (millibars)'] <= 1200)]  # Pressure range

# Handle incorrect data for non-numeric columns
valid_precip_types = ['rain', 'snow']
valid_summaries = [
    'Partly Cloudy', 'Mostly Cloudy', 'Overcast', 'Foggy', 'Clear',
    'Breezy and Mostly Cloudy', 'Breezy and Overcast', 'Breezy and Partly Cloudy',
    'Windy and Partly Cloudy', 'Windy and Overcast', 'Light Rain', 'Drizzle', 'Rain'
]
valid_daily_summaries = [
    'Partly cloudy throughout the day.', 'Mostly cloudy throughout the day.',
    'Overcast throughout the day.', 'Foggy in the morning and evening.',
    'Clear throughout the day.', 'Breezy in the afternoon and mostly cloudy starting in the afternoon, continuing until evening.',
    'Breezy in the afternoon and overcast starting in the afternoon.', 'Breezy in the afternoon and partly cloudy starting in the afternoon, continuing until evening.',
    'Windy in the afternoon and partly cloudy starting in the afternoon, continuing until evening.', 'Windy in the afternoon and overcast starting in the afternoon.',
    'Light rain throughout the day.', 'Drizzle in the morning and afternoon.', 'Rain throughout the day.'
]

df = df[df['Precip Type'].isin(valid_precip_types)]
df = df[df['Summary'].isin(valid_summaries)]
df = df[df['Daily Summary'].isin(valid_daily_summaries)]

# Handle Missing Values
df = df.dropna()

# Feature Selection (use only numerical features)
numerical_features = ['Temperature (C)', 'Apparent Temperature (C)', 'Humidity', 'Wind Speed (km/h)',
                      'Wind Bearing (degrees)', 'Visibility (km)', 'Pressure (millibars)']
df_numerical = df[numerical_features]

# Normalize/Standardize Features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df_numerical), columns=numerical_features)

# Split the Data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_scaled, df_scaled, test_size=0.2, random_state=42)

"""###**Mean Squared Error (MSE)**

- Mean Squared Error (MSE) is a commonly used metric to evaluate the performance of regression models. It measures the average squared difference between the actual (true) values and the predicted values by the model. A lower MSE indicates a better fit of the model to the data.

- In the context of our weather prediction task, MSE will help us quantify how well our model's predictions match the actual weather conditions in the test set. We will calculate MSE for each regression model we train and use it to compare their performance.


---



###**R-squared (R2)**

- R-squared (R2), also known as the coefficient of determination, is a metric that indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. It provides an indication of the goodness of fit of a model. An R2 value closer to 1 indicates that the model explains a large portion of the variance in the target variable.

- In the context of our weather prediction task, MSE will help us quantify how well our model's predictions match the actual weather conditions in the test set, while R2 will tell us how well the model's predictions explain the variance in the actual data.

"""

# Import Models
from sklearn.multioutput import MultiOutputRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# Initialize Models
base_models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(),
    'Random Forest': RandomForestRegressor()
}

multioutput_models = {name: MultiOutputRegressor(model) for name, model in base_models.items()}

# Train and Evaluate Models
from sklearn.metrics import mean_squared_error, r2_score

results = {}
for model_name, model in multioutput_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred, multioutput='uniform_average')
    r2 = r2_score(y_test, y_pred, multioutput='uniform_average')
    results[model_name] = {'MSE': mse, 'R2': r2}
    print(f"{model_name} - MSE: {mse}, R2: {r2}")

import matplotlib.pyplot as plt

best_model = multioutput_models['Linear Regression']

# Predict on the test set
y_pred_best = best_model.predict(X_test)

# Example for plotting 'Temperature (C)' predictions
plt.figure(figsize=(12, 8))

# Loop through each target feature for plotting
for i, feature in enumerate(numerical_features):
    plt.subplot(3, 3, i + 1)  # Adjust the number of rows and columns if needed
    plt.scatter(y_test[feature], y_pred_best[:, i], alpha=0.3)
    plt.plot([min(y_test[feature]), max(y_test[feature])],
             [min(y_test[feature]), max(y_test[feature])], color='red')
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.title(f'{feature}')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

best_model = multioutput_models['Random Forest']

# Predict on the test set
y_pred_best = best_model.predict(X_test)

# Example for plotting 'Temperature (C)' predictions
plt.figure(figsize=(12, 8))

# Loop through each target feature for plotting
for i, feature in enumerate(numerical_features):
    plt.subplot(3, 3, i + 1)  # Adjust the number of rows and columns if needed
    plt.scatter(y_test[feature], y_pred_best[:, i], alpha=0.3)
    plt.plot([min(y_test[feature]), max(y_test[feature])],
             [min(y_test[feature]), max(y_test[feature])], color='red')
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.title(f'{feature}')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

best_model = multioutput_models['Decision Tree']

# Predict on the test set
y_pred_best = best_model.predict(X_test)

# Example for plotting 'Temperature (C)' predictions
plt.figure(figsize=(12, 8))

# Loop through each target feature for plotting
for i, feature in enumerate(numerical_features):
    plt.subplot(3, 3, i + 1)  # Adjust the number of rows and columns if needed
    plt.scatter(y_test[feature], y_pred_best[:, i], alpha=0.3)
    plt.plot([min(y_test[feature]), max(y_test[feature])],
             [min(y_test[feature]), max(y_test[feature])], color='red')
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.title(f'{feature}')

plt.tight_layout()
plt.show()

"""####**1. What type of machine learning problem is this?**

This is a multi-output regression problem where we aim to predict multiple continuous values, such as temperature, humidity, wind speed, etc., based on historical data.


---


####**2. How do the features relate to each other and to the weather conditions?**

The features such as temperature, humidity, wind speed, and pressure are all related to weather conditions. Temperature and apparent temperature are directly related, humidity affects perceived temperature, wind speed and bearing influence weather patterns, visibility indicates clarity of the atmosphere, and pressure can signal different weather systems.


---


####**3. What are the steps involved in preprocessing the dataset for machine learning?**

The steps involved are handling missing values, selecting numerical features, normalizing/standardizing features, and splitting the dataset into training and testing sets.


---


####**4. How do we select the best model for our dataset?**

 We train different multi-output regression models (Linear Regression, Decision Tree, Random Forest) and compare their performance using metrics such as Mean Squared Error (MSE) and R-squared (R2).


---



####**5. What metrics should we use to evaluate the performance of our weather prediction model?**

Mean Squared Error (MSE) and R-squared (R2) are commonly used metrics to evaluate multi-output regression models.


---


####**6. Plot the predicted vs true values**

The answer is in the previous cells


---


###**Model Evaluation Results**

We evaluated three different multi-output regression models for the weather prediction task: Linear Regression, Decision Tree, and Random Forest. Below are the results:

- Linear Regression - MSE: 4.941507940851174e-31, R2: 1.0
- Decision Tree - MSE: 3.0332976760901136e-06, R2: 0.9999970058689884
- Random Forest - MSE: 3.7506593107141503e-06, R2: 0.999996298727481


---


###**Interpretation**

- **Linear Regression** achieved a perfect R2 score of 1.0 and an extremely low MSE of 4.941507940851174e-31. This suggests that the Linear Regression model fits the data perfectly. However, this could be an indication of overfitting if the test data is not sufficiently distinct from the training data.

- **Decision Tree** also performed exceptionally well with an MSE of
 3.0332976760901136e-06 and an R2 score of 0.9999970058689884. This indicates that the Decision Tree model is almost perfectly predicting the weather conditions.

- **Random Forest** had a slightly higher MSE of 3.7506593107141503e-06 and an R2 score of 0.999996298727481, which are still excellent results. The Random Forest model is also performing extremely well, though marginally less so than the Decision Tree in this case.

In summary, all three models showed outstanding performance in predicting the weather conditions based on the historical data. The Linear Regression model showed a perfect fit, while the Decision Tree and Random Forest models also provided nearly perfect predictions.

"""